{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJymtOMlE_Pv"
      },
      "source": [
        "**Name of Student** :- Gauri Kishor Damle\\\n",
        "**NUID**:- 002931881"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYT5Ilb4FZmS"
      },
      "outputs": [],
      "source": [
        "#### Import required libraries\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "!pip install contractions\n",
        "import contractions\n",
        "!pip install datasets\n",
        "import datasets\n",
        "\n",
        "### nltk\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords \n",
        "import string \n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import movie_reviews\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import Counter\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from random import shuffle\n",
        "\n",
        "## sklearn\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "\n",
        "## tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM, Embedding, Bidirectional, Activation, Flatten\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "\n",
        "## gensim \n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import FastText\n",
        "from gensim.corpora import Dictionary\n",
        "import gensim.downloader as api\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from gensim.models import KeyedVectors\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCeYjXWYGbyo"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Reading train and test sets "
      ],
      "metadata": {
        "id": "MOqlpHfjKB5m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcGkz9rRGb2F"
      },
      "outputs": [],
      "source": [
        "df_train_data = pd.read_csv('/content/drive/MyDrive/SemesterTwo/IE7374/Assignment 4/archive/atis_intents_train.csv', header=0, names=['intent','text'])\n",
        "df_test_data = pd.read_csv('/content/drive/MyDrive/SemesterTwo/IE7374/Assignment 4/archive/atis_intents_test.csv', header=0, names=['intent','text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDY1m8egel1-"
      },
      "outputs": [],
      "source": [
        "df_train_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ph2pd1Vyfr02"
      },
      "source": [
        "#### The above mentioned data contains 8 different intents. It has 4833 records with customer questions which can be classified into these 8 intents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVDq_l07gHFH"
      },
      "source": [
        "#### The ATIS dataset is a standard benchmark dataset widely used as an intent classification. ATIS Stands for Airline Travel Information System. Intent classification is an important component of Natural Language Understanding (NLU) systems in any chatbot platform.\n",
        "\n",
        "\n",
        "#### ATIS dataset provides large number of messages and their associated intents that can be used in training a classifier. Within a chatbot, intent refers to the goal the customer has in mind when typing in a question or comment. While entity refers to the modifier the customer uses to describe their issue, the intent is what they really mean. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGnaOPBpIino"
      },
      "outputs": [],
      "source": [
        "df_train_data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQCqJl9QnVW6"
      },
      "outputs": [],
      "source": [
        "df_train_data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bsxaqqhd1wXg"
      },
      "source": [
        "# **Common functions for Data Cleaning and Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pKlSl5U9agV"
      },
      "source": [
        "The Input data contains unclean data.\n",
        "\n",
        "To improve performance of model, lower casing, punctuation removal stop words removal is performed as part of preprocessing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNzMhJ8agM4r"
      },
      "source": [
        "#### Data processing functions "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYDOdHkY9SBo"
      },
      "outputs": [],
      "source": [
        "#### Lower Casing\n",
        "def lowerCasing(total_dataset):\n",
        "  return total_dataset.apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
        "\n",
        "#### Removing contractions\n",
        "def removeContraction(data):\n",
        "  expanded_words = []   \n",
        "  for word in data.split():\n",
        "    # using contractions.fix to expand the shortened words\n",
        "    expanded_words.append(contractions.fix(word))  \n",
        "   \n",
        "  expanded_text = ' '.join(expanded_words)\n",
        "  return expanded_text\n",
        "\n",
        "#### Punctuation Removal\n",
        "def removePunctuation(data):\n",
        "  return data.str.replace('[^\\w\\s]','')\n",
        "\n",
        "\n",
        "#### Identifying and Removing Stop Words\n",
        "def removeStopWords(data):\n",
        "  stop_words = stopwords.words('english')\n",
        "  sw_list = ['1','2','3','4','5','6','7','8','9','0']\n",
        "  stop_words.extend(sw_list)\n",
        "  return data.apply(lambda x: \" \".join(x for x in x.split() if x not in stop_words))\n",
        "\n",
        "\n",
        "## text cleaning\n",
        "#Preprovessing function to remove tags, html, special characters,and etc\n",
        "TAG_RE = re.compile(r'<[^>]+>')\n",
        "\n",
        "def remove_tags(text):\n",
        "    return TAG_RE.sub('', text)\n",
        "    \n",
        "def preprocess_text(inputSentence):\n",
        "\n",
        "    # Removing html tags\n",
        "    sentence = remove_tags(inputSentence)\n",
        "\n",
        "    # Single character removal\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "\n",
        "     # Removing email address \n",
        "    sentence = re.sub(r'[^\\s]+@[^*()\\s]+', ' ', sentence)\n",
        "\n",
        "     # Removing http address \n",
        "    sentence = re.sub(r'(http|https)://[^\\s]*', ' ', sentence)\n",
        "\n",
        "     # Removing dollar address \n",
        "    sentence = re.sub(r'[$]+', ' ', sentence)\n",
        "    \n",
        "    # Removing multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocessing training data**"
      ],
      "metadata": {
        "id": "6S_kTmn8XsfF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIgY65XoTou5"
      },
      "outputs": [],
      "source": [
        "df_train = df_train_data.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOEKISsbgSo9"
      },
      "outputs": [],
      "source": [
        "df_train['text'] = lowerCasing(df_train['text'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['text'] = df_train.text.apply(lambda x : removeContraction(x))"
      ],
      "metadata": {
        "id": "WgED1upCOyxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_Oc08kFgW0W"
      },
      "outputs": [],
      "source": [
        "df_train['text'] = removePunctuation(df_train['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKcvqDQOgemy"
      },
      "outputs": [],
      "source": [
        "df_train['text'] = removeStopWords(df_train['text'])\n",
        "\n",
        "df_train.text = df_train.text.apply(lambda x : preprocess_text(x))\n",
        "df_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnwJ75wruHSQ"
      },
      "outputs": [],
      "source": [
        "x_train = df_train.text.apply(lambda x : preprocess_text(x))\n",
        "x_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nas6vwlwHpSL"
      },
      "outputs": [],
      "source": [
        "df_train['intent'].value_counts().nunique()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['intent'].value_counts()"
      ],
      "metadata": {
        "id": "gDmDVf5pIYFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = df_train['intent']\n",
        "one_hot = MultiLabelBinarizer()\n",
        "y_train = one_hot.fit_transform(y_train)"
      ],
      "metadata": {
        "id": "RZoZmzNZFI71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "metadata": {
        "id": "k1I3ntcIdMrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfHNYz4jASI8"
      },
      "source": [
        "# **EDA on preprocessed data**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eYvUfiHeoUv"
      },
      "outputs": [],
      "source": [
        "# Word Count\n",
        "df_train['WordCount'] = df_train['text'].str.len() #Word Count Per review\n",
        "df_train.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train"
      ],
      "metadata": {
        "id": "zigxQDD3yQy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FzFBOpIg6Zo"
      },
      "source": [
        "#### Distribution of Word Counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgq0L8gKepye"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14,7))\n",
        "\n",
        "plt.hist(df_train[df_train['intent']==\"atis_flight\"]['WordCount'], bins = 30, alpha = 0.7)\n",
        "plt.hist(df_train[df_train['intent']==\"atis_airfare\"]['WordCount'], bins = 30, alpha = 0.7)\n",
        "plt.hist(df_train[df_train['intent']==\"atis_ground_service\"]['WordCount'], bins = 30, alpha = 0.7)\n",
        "plt.hist(df_train[df_train['intent']==\"atis_airline\"]['WordCount'], bins = 30, alpha = 0.7)\n",
        "plt.hist(df_train[df_train['intent']==\"atis_abbreviation\"]['WordCount'], bins = 30, alpha = 0.7)\n",
        "plt.hist(df_train[df_train['intent']==\"atis_aircraft\"]['WordCount'], bins = 30, alpha = 0.7)\n",
        "plt.hist(df_train[df_train['intent']==\"atis_flight_time\"]['WordCount'], bins = 30, alpha = 0.7)\n",
        "plt.hist(df_train[df_train['intent']==\"atis_quantity\"]['WordCount'], bins = 30, alpha = 1)\n",
        "\n",
        "\n",
        "plt.legend([\"atis_flight\",\"atis_airfare\",\"atis_ground_service\",\"atis_airline\",\"atis_abbreviation\",\"atis_aircraft\",\"atis_flight_time\",\"atis_quantity\"])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGKmFxpUrv6M"
      },
      "source": [
        "#### WordCloud generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMC8qkaPr2NB"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#Import packages\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "#Set defaults for graph formats\n",
        "plt.rcParams['figure.figsize']=(14,7) #Sets default for the size of the graph\n",
        "matplotlib.rcParams['axes.titlesize']=24 #Sets default for the size of the title\n",
        "matplotlib.rcParams['axes.labelsize']=20 #Sets default for the size of the x/y axis labels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "intent=[\"atis_abbreviation\", \"atis_aircraft\", \"atis_airfare\", \"atis_airline\",\n",
        "           \"atis_flight\", \"atis_flight_time\", \"atis_ground_service\",\"atis_quantity\"]\n",
        "\n",
        "\n",
        "x = df_train.groupby(['intent'])['WordCount'].sum().reset_index()['intent']\n",
        "y = df_train.groupby(['intent'])['WordCount'].sum().reset_index()['WordCount']\n",
        "x_pos = np.arange(len(x))\n",
        "plt.style.use('ggplot')\n",
        "barchart=plt.bar(x_pos, y, color=['#DC8458', '#950702', '#8E067D', '#2E8C44', '#395196', '#60A619','#ECA10A'])\n",
        "plt.xlabel(\"intent\")\n",
        "plt.ylabel(\"Word Count\")\n",
        "plt.title(\"Total Number of Words in Each Intent\")\n",
        "plt.xticks(x_pos, intent)\n",
        "\n",
        "# Add counts above the two bar graphs\n",
        "for bar in barchart:\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2.0, height, '%d' % int(height), ha='center', va='bottom')"
      ],
      "metadata": {
        "id": "ig86bkcu4vYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### As it can be seen in histogram and bar graph, the dataset is baised towards the ATIS flight. Overall more than 50 percent approx 78% questions are with intent of Flight."
      ],
      "metadata": {
        "id": "UCdLZQyS75dt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chart = sns.countplot(df_train.intent)\n",
        "plt.title(\"Number of texts rows per intent\")\n",
        "chart.set_xticklabels(chart.get_xticklabels(), rotation=30, horizontalalignment='right');"
      ],
      "metadata": {
        "id": "74YAATTeehOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "intent=[\"atis_abbreviation\", \"atis_aircraft\", \"atis_airfare\", \"atis_airline\",\n",
        "           \"atis_flight\", \"atis_flight_time\", \"atis_ground_service\",\"atis_quantity\"]\n",
        "\n",
        "\n",
        "plt.figure(figsize = (14, 7))\n",
        "data = df_train.intent.value_counts()\n",
        "\n",
        "ax  = data.plot.pie(autopct = '%1.1f%%', labels = data.index,  fontsize = 14)\n",
        "ax.set_title('Intent Distribution', fontsize = 18)\n",
        "plt.axis('off')\n",
        "ax.legend(labels = data.index, loc = \"upper left\", fontsize = 14, fancybox = True, labelspacing = 1, framealpha = 1, shadow=True, borderpad=1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0CeSkXvy6nTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \" \".join(i for i in x_train )\n",
        "\n",
        "wordcloud = WordCloud().generate(text)\n",
        "plt.figure( figsize=(14,7))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TX0AT-Gs5flt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocessing test data**"
      ],
      "metadata": {
        "id": "bp3ucfTFX8wM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rY0e5FgqX8wN"
      },
      "outputs": [],
      "source": [
        "df_test = df_test_data.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcR4CCqfX8wN"
      },
      "outputs": [],
      "source": [
        "df_test['text'] = lowerCasing(df_test['text'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test['text']  = df_test.text.apply(lambda x : removeContraction(x))"
      ],
      "metadata": {
        "id": "4HdoMGquX8wN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFv4LqCTX8wO"
      },
      "outputs": [],
      "source": [
        "df_test['text'] = removePunctuation(df_test['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uoK1oFKX8wO"
      },
      "outputs": [],
      "source": [
        "df_test['text'] = removeStopWords(df_test['text'])\n",
        "\n",
        "df_test.text = df_test.text.apply(lambda x : preprocess_text(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayVcEY7bX8wO"
      },
      "outputs": [],
      "source": [
        "x_test = df_test.text.apply(lambda x : preprocess_text(x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HkNKsHi6X8wO"
      },
      "outputs": [],
      "source": [
        "df_test['intent'].value_counts().nunique()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test['intent'].value_counts()"
      ],
      "metadata": {
        "id": "Nfek7iZwX8wO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test = df_test['intent']\n",
        "one = MultiLabelBinarizer()\n"
      ],
      "metadata": {
        "id": "zOSvQPTQZFM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test = one.fit_transform(y_test)"
      ],
      "metadata": {
        "id": "fOjmp9XOCydc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.shape"
      ],
      "metadata": {
        "id": "AyvleEWWh24H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZtr1dEq2uqm"
      },
      "source": [
        "# **Creating Word Vectors for Model Build**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = [d.split() for d in x_train]\n"
      ],
      "metadata": {
        "id": "BjQVfqdmd5Tz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = np.array(y_train)\n"
      ],
      "metadata": {
        "id": "TZAB8HovLMln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.dtype"
      ],
      "metadata": {
        "id": "JF-ylroEUF_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAX8E3CSnj6H"
      },
      "outputs": [],
      "source": [
        "OUTPUT_DIM = 300"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44kUdQyaoZA-"
      },
      "outputs": [],
      "source": [
        "## tenserflow keras tockenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(x_train)\n",
        "x_train = tokenizer.texts_to_sequences(x_train)\n",
        "x_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rplWrBA_mona"
      },
      "outputs": [],
      "source": [
        "len(tokenizer.word_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y80jiZu-mokp"
      },
      "outputs": [],
      "source": [
        "plt.hist([len(x) for x in x_train], bins =30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dDCq_KVmocV"
      },
      "outputs": [],
      "source": [
        "#### As per histomram the max 21 words are present in the messages, so we can keep maxlen = 21\n",
        "array = np.array([len(x) for x in x_train])\n",
        "len(array[array>21])\n",
        "\n",
        "## As we have almost all sentences with 21 word count per sentence, truncate the sequence lenght with maxlen = 21"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYp-Sl-8pUHn"
      },
      "outputs": [],
      "source": [
        "input_length = 21\n",
        "x_train = pad_sequences(x_train, maxlen=input_length)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "metadata": {
        "id": "1AG5OXdqTfXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpKKz6SYpeWB"
      },
      "outputs": [],
      "source": [
        "INPUT_VOAB_LEN = len(tokenizer.word_index)+1\n",
        "vocab = tokenizer.word_index\n",
        "INPUT_VOAB_LEN"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "x_test = [d.split() for d in x_test]\n"
      ],
      "metadata": {
        "id": "_16qmdSzlytc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test = np.array(y_test)"
      ],
      "metadata": {
        "id": "WxSBVbk6LRgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.shape"
      ],
      "metadata": {
        "id": "aw-F-GSzLcLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = tokenizer.texts_to_sequences(x_test)\n",
        "x_test"
      ],
      "metadata": {
        "id": "edKeSoA2lytc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJYNl3iQlytd"
      },
      "outputs": [],
      "source": [
        "x_test = pad_sequences(x_test, maxlen=input_length)\n",
        "x_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQidLgqL7IeJ"
      },
      "source": [
        "# **Common Functions for LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P64MoZ7ZjkxI"
      },
      "outputs": [],
      "source": [
        "##### Common functionS\n",
        "\n",
        "def show_performance_plot(mdl):\n",
        "  #show the model accuracy\n",
        "  plt.plot(mdl.history['acc'])\n",
        "  plt.plot(mdl.history['val_acc'])\n",
        "\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train','test'], loc='upper left')\n",
        "  plt.show()\n",
        "  #show model loss\n",
        "  plt.plot(mdl.history['loss'])\n",
        "  plt.plot(mdl.history['val_loss'])\n",
        "\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train','test'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def LSTM_model_build(input_dim, output_dim, embedding_vectors, input_length, x_train, y_train, x_test, y_test): \n",
        "\n",
        "  model_LSTM = Sequential()\n",
        "  model_LSTM.add(Embedding(input_dim = input_dim, output_dim = output_dim, weights = [embedding_vectors], input_length = input_length, trainable = False))\n",
        "\n",
        "\n",
        "  model_LSTM.add(LSTM(128))\n",
        "  \n",
        "\n",
        "  model_LSTM.add(Dense(128, activation='sigmoid'))\n",
        "\n",
        "\n",
        "\n",
        " \n",
        "  model_LSTM.add(Dense(21, activation='softmax'))\n",
        "  model_LSTM.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "  model_LSTM.summary()\n",
        "\n",
        "  model_LSTM_plt = model_LSTM.fit(x_train, y_train, verbose=1, validation_split =0.3, epochs=100,callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=3,\n",
        "            restore_best_weights=True)],  batch_size = 128)\n",
        "  y_pred = (model_LSTM.predict(x_test) >= 0.5).astype(int)\n",
        "  score, accuracy = model_LSTM.evaluate(x_test, y_test, verbose=2, batch_size = 128)\n",
        "  print(\"accuracy value = {}\".format(accuracy*100))\n",
        " \n",
        "  show_performance_plot(model_LSTM_plt)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "  \n",
        "\n",
        "\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mq6xKI7tjYXn"
      },
      "source": [
        "# **Pretrained Model word2vec**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvlmGPx6fQoO"
      },
      "outputs": [],
      "source": [
        "# https://code.google.com/archive/p/word2vec/\n",
        "DIR = \"/content/drive/MyDrive/SemesterTwo/IE7374/Project1/\"\n",
        "\n",
        "model_w2v = KeyedVectors.load_word2vec_format(DIR+'GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
        "print(\"word2vec model loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whyQ_jLGsOfX"
      },
      "outputs": [],
      "source": [
        "embedding_vectors_pretrained = np.zeros((INPUT_VOAB_LEN, OUTPUT_DIM))\n",
        "\n",
        "#if word in vocab.items():\n",
        "for word, i in vocab.items():\n",
        "  if word in model_w2v.wv:\n",
        "    embedding_vectors_pretrained[i] = model_w2v.wv[word]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LSTM_model_build(INPUT_VOAB_LEN, OUTPUT_DIM, embedding_vectors_pretrained, input_length, x_train, y_train, x_test, y_test)"
      ],
      "metadata": {
        "id": "EjQ79MDRUsb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlO5mTXJp4hj"
      },
      "source": [
        "# **Pretrained Model Fastext**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXHibq_xp4hk"
      },
      "outputs": [],
      "source": [
        "# https://code.google.com/archive/p/word2vec/\n",
        "DIR = \"/content/drive/MyDrive/SemesterTwo/IE7374/Project1/\"\n",
        "\n",
        "\n",
        "model_fasttext = KeyedVectors.load_word2vec_format(DIR+'wiki-news-300d-1M.vec')\n",
        "print(\"FastText model loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJX9hb6-p4hk"
      },
      "outputs": [],
      "source": [
        "# getting initial weights from model_fasttext model\n",
        "embedding_vectors_pretrained = np.zeros((INPUT_VOAB_LEN, 300))\n",
        "\n",
        "#if word in vocab.items():\n",
        "for word, i in vocab.items():\n",
        "  if word in model_fasttext.wv:\n",
        "    embedding_vectors_pretrained[i] = model_fasttext.wv[word]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BetCr2Q-rOcJ"
      },
      "outputs": [],
      "source": [
        "LSTM_model_build(INPUT_VOAB_LEN, OUTPUT_DIM, embedding_vectors_pretrained, input_length, x_train, y_train, x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGpQs6Cq-gnf"
      },
      "source": [
        "# **Pretrained Model Glove**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uPq5UWE-gnf"
      },
      "outputs": [],
      "source": [
        "# https://nlp.stanford.edu/projects/glove/\n",
        "DIR = \"/content/drive/MyDrive/SemesterTwo/IE7374/Project1/\"\n",
        "\n",
        "embeddings_index = {}\n",
        "\n",
        "# Pre-trained Glove\n",
        "#if option == 1:\n",
        "with open(os.path.join(DIR, 'glove.6B.300d.txt')) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, 'f', sep=' ')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4egKgLbC-gnf"
      },
      "outputs": [],
      "source": [
        "\n",
        "print('Preparing embedding matrix.')\n",
        "# prepare embedding matrix\n",
        "\n",
        "embedding_matrix = np.zeros((INPUT_VOAB_LEN, OUTPUT_DIM))\n",
        "for word, i in vocab.items():\n",
        "\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "print(embedding_matrix.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5tg_-Mi-gnf"
      },
      "outputs": [],
      "source": [
        "LSTM_model_build(INPUT_VOAB_LEN, OUTPUT_DIM, embedding_matrix, input_length, x_train, y_train, x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bert Model**"
      ],
      "metadata": {
        "id": "ljbkWD1krG0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "z_70kDnRgfvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "checkpoint = 'bert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "ptol9lKSgTF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.vocab)"
      ],
      "metadata": {
        "id": "2KeCRQvYlTYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#size of vocabulary\n",
        "print(len(tokenizer.vocab))"
      ],
      "metadata": {
        "id": "k7IIQZFtovjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"I like NLP\"\n",
        "print(sentence)\n",
        "\n",
        "# bert model uses word piece tokenization GPT2 uses byte pair encoding tokenization\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "print(tokens)\n",
        "\n",
        "\n",
        "ids = tokenizer.encode(sentence)\n",
        "print(ids)\n",
        "print(tokenizer.decode(ids))"
      ],
      "metadata": {
        "id": "tVt8fWfiYMu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## checking for class imbalance in multiclass classification\n",
        "df_train_data['intent'].value_counts()"
      ],
      "metadata": {
        "id": "WQ5N8uOJYNdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "## use GPU for faster runtime\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "## for multiclass classification\n",
        "num_labels = 8 \n",
        "\n",
        "model = (\n",
        "    AutoModelForSequenceClassification.from_pretrained(\n",
        "        checkpoint,\n",
        "        num_labels = num_labels\n",
        "\n",
        "    ).to(device))"
      ],
      "metadata": {
        "id": "HRS4qd3Mau_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bert_tokenize_function(batch):\n",
        "  return tokenizer(batch[\"text\"], padding = True, truncation=True)"
      ],
      "metadata": {
        "id": "UJTaoGa4YODS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train[\"label\"] = df_train[\"intent\"]\n",
        "df_train = df_train.drop(\"intent\", axis=1)\n"
      ],
      "metadata": {
        "id": "f2Cdc-ZxVZVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()\n",
        "df_train[\"label\"] = le.fit_transform(df_train[\"label\"])\n",
        "\n"
      ],
      "metadata": {
        "id": "55dXhKJAcOqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train"
      ],
      "metadata": {
        "id": "N8rkMeNUgpWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test[\"label\"] = df_test[\"intent\"]\n",
        "df_test = df_test.drop(\"intent\", axis=1)"
      ],
      "metadata": {
        "id": "OGw6w_O8Wf6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()\n",
        "df_test[\"label\"] = le.fit_transform(df_test[\"label\"])\n",
        "\n"
      ],
      "metadata": {
        "id": "gbAj7V2igslz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "train_dataset = Dataset.from_dict(df_train)\n",
        "test_dataset = Dataset.from_dict(df_test)\n",
        "validation_dataset = Dataset.from_dict(df_train.tail(100))\n",
        "\n",
        "my_dataset_dict = datasets.DatasetDict({\"train\":train_dataset,\"test\":test_dataset, \"validation\":validation_dataset})"
      ],
      "metadata": {
        "id": "m0dEKGVPfOi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "encoded_dataset_test = my_dataset_dict.map(bert_tokenize_function, batched=True, batch_size=None)"
      ],
      "metadata": {
        "id": "pROycBPVfM1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_dataset_test"
      ],
      "metadata": {
        "id": "j3BB_niQT-zX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_dataset_test['train'][0]\n",
        "    \n"
      ],
      "metadata": {
        "id": "MxoYdw9_pe1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import trainer, TrainingArguments\n",
        "\n",
        "batch_size = 8\n",
        "logging_steps = len(encoded_dataset_test[\"train\"])"
      ],
      "metadata": {
        "id": "UY0CJxeOs3V8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name =f\"{checkpoint}-finetuned-model\"\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = model_name,\n",
        "    num_train_epochs = 2,\n",
        "    learning_rate = 2e-5,\n",
        "    per_device_train_batch_size = batch_size,\n",
        "    per_device_eval_batch_size = batch_size,\n",
        "    weight_decay = 0.01,\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    disable_tqdm = False,\n",
        "    logging_steps = logging_steps,\n",
        "    log_level = \"error\",\n",
        "    optim = 'adamw_torch'\n",
        ")"
      ],
      "metadata": {
        "id": "-uwXUIRqs38o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "trainer = Trainer(model = model,\n",
        "                  args = training_args,\n",
        "                  train_dataset = encoded_dataset_test['train'],\n",
        "                  eval_dataset = encoded_dataset_test['validation'],\n",
        "                  tokenizer = tokenizer)\n",
        "\n"
      ],
      "metadata": {
        "id": "wGOufCuXs4mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "UrrLdkdUXbRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.predict(encoded_dataset_test['test'])"
      ],
      "metadata": {
        "id": "9r4K4On6W72B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = trainer.predict(encoded_dataset_test['test'])\n",
        "preds\n"
      ],
      "metadata": {
        "id": "jtWs2w2biJKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds.predictions.shape"
      ],
      "metadata": {
        "id": "4Q7OztNCiMJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy(preds):\n",
        "  predictions = preds.predictions.argmax(axis=-1)\n",
        "  labels = preds.label_ids\n",
        "  accuracy = accuracy_score(preds.label_ids, preds.predictions.argmax(axis=-1))\n",
        "  return {'accuracy': accuracy}"
      ],
      "metadata": {
        "id": "b2Kn3ry4iPCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "trainer = Trainer(model=model, \n",
        "                  compute_metrics=get_accuracy,\n",
        "                  args=training_args, \n",
        "                  train_dataset=encoded_dataset_test[\"train\"],\n",
        "                  eval_dataset = encoded_dataset_test['validation'],\n",
        "                  tokenizer=tokenizer)\n",
        "trainer.train();"
      ],
      "metadata": {
        "id": "a3Ce-Znsng6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training run"
      ],
      "metadata": {
        "id": "Eu0paBWpnzOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "logging_steps = len(encoded_dataset_test[\"train\"]) // batch_size\n",
        "model_name = f\"{checkpoint}-finetuned-imdb\"\n",
        "training_args = TrainingArguments(output_dir=model_name,\n",
        "                                  num_train_epochs=2,\n",
        "                                  learning_rate=2e-5,\n",
        "                                  per_device_train_batch_size=batch_size,\n",
        "                                  per_device_eval_batch_size = batch_size,\n",
        "                                  weight_decay=0.01,\n",
        "                                  evaluation_strategy=\"epoch\",\n",
        "                                  disable_tqdm=False,\n",
        "                                  logging_steps=logging_steps,\n",
        "                                  log_level=\"error\",\n",
        "                                  optim='adamw_torch'\n",
        "                                  )"
      ],
      "metadata": {
        "id": "tyHI4AYUnhYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "trainer = Trainer(model=model, \n",
        "                  args=training_args, \n",
        "                  compute_metrics=get_accuracy,\n",
        "                  train_dataset=encoded_dataset_test[\"train\"],\n",
        "                  eval_dataset = encoded_dataset_test['validation'],\n",
        "                  tokenizer=tokenizer)\n",
        "trainer.train();"
      ],
      "metadata": {
        "id": "RyiVPzotnhvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "lexevouqoHAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model()"
      ],
      "metadata": {
        "id": "5apfKcXGoJ4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name"
      ],
      "metadata": {
        "id": "v8Qas0aSoMt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "classifier = pipeline('text-classification', model=model_name)\n",
        "classifier('what flights are available from here')"
      ],
      "metadata": {
        "id": "xu0JD42LoPZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier('This was ticket ')"
      ],
      "metadata": {
        "id": "khYNn0NNoQ42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4CnbFv3qMkI"
      },
      "source": [
        "\n",
        "# **Final Conclusion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYUW0RgsqSrj"
      },
      "source": [
        "#### Accuracy comparison for following models\n",
        "#### LSTM model with following word embedding vectors\n",
        "\n",
        "#### Word2Vec\n",
        "\n",
        "1.   Pretrained - 90%\n",
        "\n",
        "#### FastText\n",
        "\n",
        "1.   Pretrained - 94%\n",
        "\n",
        "#### Glove Pretrained\n",
        "\n",
        "1.   Pretrained - 89%\n",
        "\n",
        "\n",
        "#### Bert - simpletransformer word embeddings\n",
        "\n",
        "1.   Pretrained - 97%\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the size of data increases, the colab can crash while bulding models. Thus to improve model performance in such cases, we can shift the processing to GPU processor for faster results."
      ],
      "metadata": {
        "id": "Xe8jz0uM93w6"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Bsxaqqhd1wXg",
        "DNzMhJ8agM4r",
        "6S_kTmn8XsfF",
        "MfHNYz4jASI8",
        "bp3ucfTFX8wM",
        "lZtr1dEq2uqm",
        "FQidLgqL7IeJ",
        "mq6xKI7tjYXn",
        "qlO5mTXJp4hj",
        "bGpQs6Cq-gnf"
      ],
      "name": "BertTextClassification.ipynb",
      "provenance": [],
      "private_outputs": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}